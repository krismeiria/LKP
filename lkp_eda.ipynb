{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d000c62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from pdf2image import convert_from_bytes\n",
    "import pytesseract\n",
    "from openai import OpenAI\n",
    "\n",
    "# =============================================\n",
    "# CONFIG\n",
    "# =============================================\n",
    "\n",
    "TOTAL_PAGES = 1261\n",
    "CHUNK_SIZE = 100\n",
    "\n",
    "DETAIL_WORKERS = 20\n",
    "OCR_WORKERS = 10\n",
    "OPENAI_WORKERS = 3   # keep small\n",
    "\n",
    "BASE_LIST_URL = \"https://kursus.kemendikdasmen.go.id/?page={}\"\n",
    "DETAIL_BASE = \"https://referensi.data.kemendikdasmen.go.id/pendidikan/npsn/{}\"\n",
    "\n",
    "OUTPUT_DIR = \"output\"\n",
    "CACHE_FILE = \"program_cache.json\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\",\n",
    "    \"Referer\": \"https://referensi.data.kemendikdasmen.go.id/\"\n",
    "}\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Optional for Mac\n",
    "# pytesseract.pytesseract.tesseract_cmd = \"/opt/homebrew/bin/tesseract\"\n",
    "\n",
    "# =============================================\n",
    "# LOAD CACHE\n",
    "# =============================================\n",
    "\n",
    "if os.path.exists(CACHE_FILE):\n",
    "    with open(CACHE_FILE, \"r\") as f:\n",
    "        program_cache = json.load(f)\n",
    "else:\n",
    "    program_cache = {}\n",
    "\n",
    "# =============================================\n",
    "# RESUME LOGIC\n",
    "# =============================================\n",
    "\n",
    "pattern = r\"lkp_(\\d+)_(\\d+)\\.csv\"\n",
    "completed = []\n",
    "\n",
    "for file in os.listdir(OUTPUT_DIR):\n",
    "    match = re.match(pattern, file)\n",
    "    if match:\n",
    "        completed.append(int(match.group(2)))\n",
    "\n",
    "if completed:\n",
    "    start_page = max(completed) + 1\n",
    "else:\n",
    "    start_page = 1\n",
    "\n",
    "print(f\"Resuming from page {start_page}\")\n",
    "\n",
    "# =============================================\n",
    "# OCR FUNCTION\n",
    "# =============================================\n",
    "\n",
    "def ocr_pdf(sk_url):\n",
    "    try:\n",
    "        pdf_response = requests.get(sk_url, timeout=20)\n",
    "        pdf_bytes = pdf_response.content\n",
    "        images = convert_from_bytes(pdf_bytes)\n",
    "\n",
    "        text = \"\"\n",
    "        for img in images:\n",
    "            text += pytesseract.image_to_string(img, lang=\"ind\") + \"\\n\"\n",
    "\n",
    "        return text[:8000]\n",
    "\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "# =============================================\n",
    "# OPENAI FUNCTION\n",
    "# =============================================\n",
    "\n",
    "def extract_programs(npsn, sk_url):\n",
    "\n",
    "    if not sk_url:\n",
    "        return []\n",
    "\n",
    "    if npsn in program_cache:\n",
    "        return program_cache[npsn]\n",
    "\n",
    "    text = ocr_pdf(sk_url)\n",
    "\n",
    "    if not text.strip():\n",
    "        program_cache[npsn] = []\n",
    "        return []\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Extract ONLY the list of training programs offered in this Indonesian SK document.\n",
    "Return ONLY a JSON array of strings.\n",
    "If none found, return [].\n",
    "\n",
    "Document:\n",
    "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4.1-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Extract structured training program names.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0\n",
    "            )\n",
    "\n",
    "            programs = json.loads(response.choices[0].message.content.strip())\n",
    "            program_cache[npsn] = programs\n",
    "\n",
    "            with open(CACHE_FILE, \"w\") as f:\n",
    "                json.dump(program_cache, f)\n",
    "\n",
    "            return programs\n",
    "\n",
    "        except:\n",
    "            time.sleep(2)\n",
    "\n",
    "    return []\n",
    "\n",
    "# =============================================\n",
    "# MAIN LOOP BY CHUNK\n",
    "# =============================================\n",
    "\n",
    "for chunk_start in range(start_page, TOTAL_PAGES + 1, CHUNK_SIZE):\n",
    "\n",
    "    chunk_end = min(chunk_start + CHUNK_SIZE - 1, TOTAL_PAGES)\n",
    "    print(f\"\\nProcessing pages {chunk_start}-{chunk_end}\")\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # LIST SCRAPE\n",
    "    # ----------------------------------------\n",
    "\n",
    "    list_records = []\n",
    "\n",
    "    for page in tqdm(range(chunk_start, chunk_end + 1),\n",
    "                     desc=\"List Pages\"):\n",
    "        response = requests.get(BASE_LIST_URL.format(page),\n",
    "                                headers=HEADERS)\n",
    "        soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "        table = soup.find(\"table\")\n",
    "        if not table:\n",
    "            continue\n",
    "\n",
    "        rows = table.find_all(\"tr\")[1:]\n",
    "\n",
    "        for row in rows:\n",
    "            cols = row.find_all(\"td\")\n",
    "            if len(cols) >= 6:\n",
    "                list_records.append({\n",
    "                    \"NPSN\": cols[0].get_text(strip=True),\n",
    "                    \"Nama LKP\": cols[1].get_text(strip=True),\n",
    "                    \"Provinsi (List)\": cols[2].get_text(strip=True),\n",
    "                    \"Kabupaten (List)\": cols[3].get_text(strip=True),\n",
    "                    \"Kecamatan (List)\": cols[4].get_text(strip=True),\n",
    "                    \"Status (List)\": cols[5].get_text(strip=True)\n",
    "                })\n",
    "\n",
    "    list_df = pd.DataFrame(list_records)\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # DETAIL SCRAPE (PARALLEL)\n",
    "    # ----------------------------------------\n",
    "\n",
    "    def scrape_detail(npsn):\n",
    "        try:\n",
    "            url = DETAIL_BASE.format(npsn)\n",
    "            response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "            soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "            data = {\"NPSN\": npsn}\n",
    "\n",
    "            for container in soup.find_all(\"div\", class_=\"tabby-content\"):\n",
    "                rows = container.find_all(\"tr\")\n",
    "                for row in rows:\n",
    "                    cols = row.find_all(\"td\")\n",
    "\n",
    "                    if len(cols) >= 4:\n",
    "                        key = cols[1].get_text(strip=True)\n",
    "\n",
    "                        if \"File SK Operasional\" in key:\n",
    "                            link_tag = cols[3].find(\"a\", href=True)\n",
    "                            value = link_tag[\"href\"] if link_tag else None\n",
    "                        else:\n",
    "                            value = cols[3].get_text(strip=True)\n",
    "\n",
    "                        if key:\n",
    "                            data[key] = value\n",
    "\n",
    "            return data\n",
    "\n",
    "        except:\n",
    "            return {\"NPSN\": npsn}\n",
    "\n",
    "    detail_records = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=DETAIL_WORKERS) as executor:\n",
    "        futures = {\n",
    "            executor.submit(scrape_detail, npsn): npsn\n",
    "            for npsn in list_df[\"NPSN\"]\n",
    "        }\n",
    "\n",
    "        for future in tqdm(as_completed(futures),\n",
    "                           total=len(futures),\n",
    "                           desc=\"Detail Pages\"):\n",
    "            detail_records.append(future.result())\n",
    "\n",
    "    detail_df = pd.DataFrame(detail_records)\n",
    "\n",
    "    final_df = list_df.merge(detail_df, on=\"NPSN\", how=\"left\")\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # PROGRAM EXTRACTION (CONTROLLED)\n",
    "    # ----------------------------------------\n",
    "\n",
    "    programs_map = {}\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=OPENAI_WORKERS) as executor:\n",
    "        futures = {\n",
    "            executor.submit(\n",
    "                extract_programs,\n",
    "                row[\"NPSN\"],\n",
    "                row.get(\"File SK Operasional ()\")\n",
    "            ): row[\"NPSN\"]\n",
    "            for _, row in final_df.iterrows()\n",
    "        }\n",
    "\n",
    "        for future in tqdm(as_completed(futures),\n",
    "                           total=len(futures),\n",
    "                           desc=\"Programs\"):\n",
    "            programs_map[futures[future]] = future.result()\n",
    "\n",
    "    final_df[\"Programs\"] = final_df[\"NPSN\"].map(programs_map)\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # SAVE CHUNK\n",
    "    # ----------------------------------------\n",
    "\n",
    "    output_path = os.path.join(\n",
    "        OUTPUT_DIR,\n",
    "        f\"lkp_{chunk_start}_{chunk_end}.csv\"\n",
    "    )\n",
    "\n",
    "    final_df.to_csv(output_path,\n",
    "                    index=False,\n",
    "                    encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"Saved {output_path}\")\n",
    "\n",
    "print(\"\\nALL DONE.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175f0c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================================\n",
    "# CONFIG\n",
    "# =============================================\n",
    "\n",
    "OUTPUT_DIR = \"output\"\n",
    "\n",
    "CATEGORIES = [\n",
    "    \"Tata_Rias_dan_Kecantikan\",\n",
    "    \"Tata_Busana\",\n",
    "    \"Tata_Boga_Memasak\",\n",
    "    \"Teknik_Komputer\",\n",
    "    \"Teknik_Non_Komputer\",\n",
    "    \"Bahasa\",\n",
    "    \"Bimbel_Kursus_Pendidikan\",\n",
    "    \"Hotel_Pariwisata\",\n",
    "    \"Seni_dan_Budaya\",\n",
    "    \"Lainnya\"\n",
    "]\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# =============================================\n",
    "# STEP 1 — MERGE ALL FILES\n",
    "# =============================================\n",
    "\n",
    "files = [\n",
    "    os.path.join(OUTPUT_DIR, f)\n",
    "    for f in os.listdir(OUTPUT_DIR)\n",
    "    if f.endswith(\".csv\")\n",
    "]\n",
    "\n",
    "df_list = [pd.read_csv(f) for f in files]\n",
    "final_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# =============================================\n",
    "# STEP 2 — FIX PROGRAMS COLUMN\n",
    "# =============================================\n",
    "\n",
    "def safe_eval(x):\n",
    "    try:\n",
    "        return ast.literal_eval(x) if isinstance(x, str) else []\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "final_df[\"Programs\"] = final_df[\"Programs\"].apply(safe_eval)\n",
    "\n",
    "\n",
    "\n",
    "# =============================================\n",
    "# STEP 4 — CLASSIFICATION FUNCTION\n",
    "# =============================================\n",
    "\n",
    "def classify_programs(program_list):\n",
    "\n",
    "    if not program_list:\n",
    "        return {cat: 0 for cat in CATEGORIES}\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Classify these Indonesian training programs:\n",
    "\n",
    "{program_list}\n",
    "\n",
    "Into these categories:\n",
    "{CATEGORIES}\n",
    "\n",
    "Return ONLY valid JSON like:\n",
    "{{\n",
    "  \"Tata_Rias_dan_Kecantikan\": 0 or 1,\n",
    "  ...\n",
    "}}\n",
    "\n",
    "Use 1 if at least one program fits category.\n",
    "Otherwise 0.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4.1-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Classify training programs.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        return json.loads(response.choices[0].message.content.strip())\n",
    "\n",
    "    except:\n",
    "        return {cat: 0 for cat in CATEGORIES}\n",
    "\n",
    "# =============================================\n",
    "# STEP 5 — RUN CLASSIFICATION\n",
    "# =============================================\n",
    "\n",
    "results = []\n",
    "\n",
    "for programs in tqdm(final_df[\"Programs\"], desc=\"Classifying\"):\n",
    "    results.append(classify_programs(programs))\n",
    "\n",
    "classification_df = pd.DataFrame(results)\n",
    "\n",
    "final_df = pd.concat([final_df, classification_df], axis=1)\n",
    "\n",
    "# =============================================\n",
    "# OUTPUT\n",
    "# =============================================\n",
    "\n",
    "print(\"\\nCLASSIFIED SAMPLE:\")\n",
    "print(final_df[[\"Programs\"] + CATEGORIES])\n",
    "\n",
    "final_df\n",
    "\n",
    "# =============================================\n",
    "# SAVE SAMPLE TO CSV\n",
    "# =============================================\n",
    "\n",
    "final_df.to_csv(\n",
    "    \"lkp_all.csv\",\n",
    "    index=False,\n",
    "    encoding=\"utf-8-sig\"\n",
    ")\n",
    "\n",
    "print(\"\\nSaved to lkp_all.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a933b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff39fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================================\n",
    "# CONFIG\n",
    "# =============================================\n",
    "\n",
    "INPUT_DIR = \"output\"\n",
    "OUTPUT_DIR = \"output_2\"\n",
    "BATCH_SIZE = 500\n",
    "\n",
    "CATEGORIES = [\n",
    "    \"Tata_Rias_dan_Kecantikan\",\n",
    "    \"Tata_Busana\",\n",
    "    \"Tata_Boga_Memasak\",\n",
    "    \"Teknik_Komputer\",\n",
    "    \"Teknik_Non_Komputer\",\n",
    "    \"Bahasa\",\n",
    "    \"Bimbel_Kursus_Pendidikan\",\n",
    "    \"Hotel_Pariwisata\",\n",
    "    \"Seni_dan_Budaya\",\n",
    "    \"Lainnya\"\n",
    "]\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# =============================================\n",
    "# STEP 1 — MERGE ALL FILES\n",
    "# =============================================\n",
    "\n",
    "files = [\n",
    "    os.path.join(INPUT_DIR, f)\n",
    "    for f in os.listdir(INPUT_DIR)\n",
    "    if f.endswith(\".csv\")\n",
    "]\n",
    "files = sorted(files)\n",
    "\n",
    "df_list = [pd.read_csv(f) for f in files]\n",
    "final_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# =============================================\n",
    "# STEP 2 — FIX PROGRAMS COLUMN\n",
    "# =============================================\n",
    "\n",
    "def safe_eval(x):\n",
    "    try:\n",
    "        return ast.literal_eval(x) if isinstance(x, str) else []\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "final_df[\"Programs\"] = final_df[\"Programs\"].apply(safe_eval)\n",
    "\n",
    "print(\"Total rows:\", len(final_df))\n",
    "\n",
    "# =============================================\n",
    "# SAFE JSON PARSER (prevents all-0 due to JSON noise)\n",
    "# =============================================\n",
    "\n",
    "def safe_json_parse(raw: str):\n",
    "    raw = (raw or \"\").strip()\n",
    "    raw = raw.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "    try:\n",
    "        return json.loads(raw)\n",
    "    except:\n",
    "        m = re.search(r\"\\{.*\\}\", raw, re.DOTALL)\n",
    "        if m:\n",
    "            try:\n",
    "                return json.loads(m.group())\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return {cat: 0 for cat in CATEGORIES}\n",
    "\n",
    "# =============================================\n",
    "# STEP 3 — CLASSIFICATION FUNCTION\n",
    "# =============================================\n",
    "\n",
    "def classify_programs(program_list):\n",
    "\n",
    "    if not program_list:\n",
    "        return {cat: 0 for cat in CATEGORIES}\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Classify these Indonesian training programs:\n",
    "\n",
    "{program_list}\n",
    "\n",
    "Into these categories:\n",
    "{CATEGORIES}\n",
    "\n",
    "Return STRICT JSON ONLY with these exact keys:\n",
    "{{\n",
    "  \"Tata_Rias_dan_Kecantikan\": 0,\n",
    "  \"Tata_Busana\": 0,\n",
    "  \"Tata_Boga_Memasak\": 0,\n",
    "  \"Teknik_Komputer\": 0,\n",
    "  \"Teknik_Non_Komputer\": 0,\n",
    "  \"Bahasa\": 0,\n",
    "  \"Bimbel_Kursus_Pendidikan\": 0,\n",
    "  \"Hotel_Pariwisata\": 0,\n",
    "  \"Seni_dan_Budaya\": 0,\n",
    "  \"Lainnya\": 0\n",
    "}}\n",
    "\n",
    "Use 1 if at least one program fits category, else 0.\n",
    "No explanation. No extra text.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4.1-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a strict JSON classifier.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        raw = response.choices[0].message.content.strip()\n",
    "        parsed = safe_json_parse(raw)\n",
    "\n",
    "        # Ensure all keys exist\n",
    "        for cat in CATEGORIES:\n",
    "            parsed.setdefault(cat, 0)\n",
    "\n",
    "        # Force 0/1 ints\n",
    "        parsed = {cat: int(parsed.get(cat, 0) == 1) for cat in CATEGORIES}\n",
    "        return parsed\n",
    "\n",
    "    except Exception as e:\n",
    "        return {cat: 0 for cat in CATEGORIES}\n",
    "\n",
    "# =============================================\n",
    "# STEP 4 — PROCESS IN BATCHES & SAVE PER 500\n",
    "# =============================================\n",
    "\n",
    "total_rows = len(final_df)\n",
    "total_batches = math.ceil(total_rows / BATCH_SIZE)\n",
    "\n",
    "for batch_idx in range(total_batches):\n",
    "    start = batch_idx * BATCH_SIZE\n",
    "    end = min((batch_idx + 1) * BATCH_SIZE, total_rows)\n",
    "\n",
    "    batch_df = final_df.iloc[start:end].copy()\n",
    "\n",
    "    results = []\n",
    "    for programs in tqdm(\n",
    "        batch_df[\"Programs\"],\n",
    "        desc=f\"Classifying rows {start+1}-{end} (Batch {batch_idx+1}/{total_batches})\"\n",
    "    ):\n",
    "        results.append(classify_programs(programs))\n",
    "\n",
    "    classification_df = pd.DataFrame(results)[CATEGORIES]\n",
    "    batch_out = pd.concat([batch_df.reset_index(drop=True),\n",
    "                           classification_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    out_path = os.path.join(OUTPUT_DIR, f\"lkp_{start+1}_{end}.csv\")\n",
    "    batch_out.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"\\nSaved: {out_path}\")\n",
    "    print(\"Category totals in this batch:\")\n",
    "    print(batch_out[CATEGORIES].sum())\n",
    "\n",
    "print(\"\\nDONE. All batches saved to:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fadd807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================\n",
    "# CONFIG\n",
    "# =============================================\n",
    "INPUT_FOLDER = \"output_2\"\n",
    "OUTPUT_FILE = \"lkp_all.csv\"\n",
    "\n",
    "# =============================================\n",
    "# STEP 1 — COLLECT & SORT FILES\n",
    "# =============================================\n",
    "files = [\n",
    "    os.path.join(INPUT_FOLDER, f)\n",
    "    for f in os.listdir(INPUT_FOLDER)\n",
    "    if f.endswith(\".csv\")\n",
    "]\n",
    "files = sorted(files)\n",
    "\n",
    "print(\"Files found:\", len(files))\n",
    "if not files:\n",
    "    raise FileNotFoundError(f\"No CSV files found in: {INPUT_FOLDER}\")\n",
    "\n",
    "# =============================================\n",
    "# STEP 2 — MERGE\n",
    "# =============================================\n",
    "df_list = [pd.read_csv(f) for f in files]\n",
    "final_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "print(\"Merged shape:\", final_df.shape)\n",
    "\n",
    "# =============================================\n",
    "# STEP 3 — SAVE\n",
    "# =============================================\n",
    "final_df.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"Saved merged file: {OUTPUT_FILE}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
